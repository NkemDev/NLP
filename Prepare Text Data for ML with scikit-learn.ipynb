{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction from text files with scikit-learn\n",
    "\n",
    "The sklearn.feature_extraction module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image. Reference: http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Loading features from dicts - DictVectorizer\n",
    "DictVectorizer transforms lists of feature-value mappings to vectors.\n",
    "\n",
    "This transformer turns lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays or scipy.sparse matrices for use with scikit-learn estimators.\n",
    "\n",
    "The class DictVectorizer can be used to convert feature arrays represented as lists of standard Python dict objects to the NumPy/SciPy representation used by scikit-learn estimators.\n",
    "\n",
    "DictVectorizer implements “one-hot” coding for categorical (aka nominal, discrete) features. Categorical features are “attribute-value” pairs where the value is restricted to a list of discrete of possibilities without ordering (e.g. topic identifiers, types of objects, tags, names…)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "measurements = [\n",
    "    {'city': 'Dubai', 'temperature': 33.},\n",
    "    {'city': 'London', 'temperature': 12.},\n",
    "     {'city': 'San Francisco', 'temperature': 18.},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dvec = DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (0, 3)\t33.0\n",
      "  (1, 1)\t1.0\n",
      "  (1, 3)\t12.0\n",
      "  (2, 2)\t1.0\n",
      "  (2, 3)\t18.0\n"
     ]
    }
   ],
   "source": [
    "## Learn a list of feature name -> indices mappings and transform X.\n",
    "X_train_dic_counts = dvec.fit_transform(measurements)\n",
    "print(X_train_dic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.,  0.,  0., 33.],\n",
       "        [ 0.,  1.,  0., 12.],\n",
       "        [ 0.,  0.,  1., 18.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dic_counts.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'city=Dubai': 1.0, 'temperature': 33.0},\n",
       " {'city=London': 1.0, 'temperature': 12.0},\n",
       " {'city=San Francisco': 1.0, 'temperature': 18.0}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvec.inverse_transform(X_train_dic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.,  0., 22.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transform feature->value dicts to array or sparse matrix.\n",
    "## Named features not encountered during fit or fit_transform will be ignored.\n",
    "dvec.transform({'temperature': 22.0, 'city=Toronto': 1.0}).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "dvect = DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. 33.]\n",
      " [ 0.  1.  0. 12.]\n",
      " [ 0.  0.  1. 18.]]\n"
     ]
    }
   ],
   "source": [
    "X_train_dic_counts = dvec.fit_transform(measurements)\n",
    "print(X_train_dic_counts.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "support = SelectKBest(chi2, k=2).fit(X_train_dic_counts, [0, 0, 1])  ##Select features according to the k highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvec.restrict(support.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city=San Francisco', 'temperature']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Bag-of-Words Model\n",
    "Reference: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/  \n",
    "\n",
    "We may want to perform classification of documents, so each document is an “input” and a class label is the “output” for our predictive algorithm. Algorithms take vectors of numbers as input, therefore we need to convert documents to fixed-length vectors of numbers.\n",
    "\n",
    "A simple and effective model for thinking about text documents in machine learning is called the Bag-of-Words Model, or BoW.\n",
    "\n",
    "The model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in a document.\n",
    "\n",
    "This can be done by assigning each word a unique number. Then any document we see can be encoded as a fixed-length vector with the length of the vocabulary of known words. The value in each position in the vector could be filled with a count or frequency of each word in the encoded document.\n",
    "\n",
    "This is the bag of words model, where we are only concerned with encoding schemes that represent what words are present or the degree to which they are present in encoded documents without any information about order.\n",
    "\n",
    "There are many ways to extend this simple method, both by better clarifying what a “word” is and in defining what to encode about each word in the vector.\n",
    "\n",
    "The scikit-learn library provides 3 different schemes that we can use, and we will briefly look at each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Word Counts with Count Vectorizer\n",
    "\n",
    "The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n",
    "\n",
    "Create an instance of the CountVectorizer class.\n",
    "Call the fit() function in order to learn a vocabulary from one or more documents.\n",
    "Call the transform() function on one or more documents as needed to encode each as a vector.\n",
    "An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.\n",
    "\n",
    "Because these vectors will contain a lot of zeros, we call them sparse. Python provides an efficient way of handling sparse vectors in the scipy.sparse package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of text documents\n",
    "ps23 = ['The LORD is my shepherd, I lack nothing. He makes me lie down in green pastures, he leads me beside quiet waters, he refreshes my soul. He guides me along the right paths for his name’s sake. Even though I walk through the darkest valley,I will fear no evil, for you are with me; your rod and your staff, they comfort me. You prepare a table before me in the presence of my enemies. You anoint my head with oil; my cup overflows. Surely your goodness and love will follow me all the days of my life, and I will dwell in the house of the LORD forever.']\n",
    "# create the transform\n",
    "cvec = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "ps23_Counts = cvec.fit_transform(ps23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 69)\n",
      "{'the': 58, 'lord': 33, 'is': 28, 'my': 37, 'shepherd': 53, 'lack': 29, 'nothing': 40, 'he': 23, 'makes': 35, 'me': 36, 'lie': 31, 'down': 11, 'in': 27, 'green': 21, 'pastures': 44, 'leads': 30, 'beside': 6, 'quiet': 48, 'waters': 64, 'refreshes': 49, 'soul': 54, 'guides': 22, 'along': 1, 'right': 50, 'paths': 45, 'for': 18, 'his': 25, 'name': 38, 'sake': 52, 'even': 14, 'though': 60, 'walk': 63, 'through': 61, 'darkest': 9, 'valley': 62, 'will': 65, 'fear': 16, 'no': 39, 'evil': 15, 'you': 67, 'are': 4, 'with': 66, 'your': 68, 'rod': 51, 'and': 2, 'staff': 55, 'they': 59, 'comfort': 7, 'prepare': 46, 'table': 57, 'before': 5, 'presence': 47, 'of': 41, 'enemies': 13, 'anoint': 3, 'head': 24, 'oil': 42, 'cup': 8, 'overflows': 43, 'surely': 56, 'goodness': 20, 'love': 34, 'follow': 17, 'all': 0, 'days': 10, 'life': 32, 'dwell': 12, 'house': 26, 'forever': 19}\n",
      "['all', 'along', 'and', 'anoint', 'are', 'before', 'beside', 'comfort', 'cup', 'darkest', 'days', 'down', 'dwell', 'enemies', 'even', 'evil', 'fear', 'follow', 'for', 'forever', 'goodness', 'green', 'guides', 'he', 'head', 'his', 'house', 'in', 'is', 'lack', 'leads', 'lie', 'life', 'lord', 'love', 'makes', 'me', 'my', 'name', 'no', 'nothing', 'of', 'oil', 'overflows', 'pastures', 'paths', 'prepare', 'presence', 'quiet', 'refreshes', 'right', 'rod', 'sake', 'shepherd', 'soul', 'staff', 'surely', 'table', 'the', 'they', 'though', 'through', 'valley', 'walk', 'waters', 'will', 'with', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "# summarize\n",
    "print(ps23_Counts.shape)\n",
    "print(cvec.vocabulary_) ## This lists all the words and the corresponding indices. \n",
    "print(cvec.get_feature_names()) ## This lists the list of words aka bag of words in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "['all', 'along', 'and', 'anoint', 'are', 'before', 'beside', 'comfort', 'cup', 'darkest', 'days', 'down', 'dwell', 'enemies', 'even', 'evil', 'fear', 'follow', 'for', 'forever', 'goodness', 'green', 'guides', 'he', 'head', 'his', 'house', 'in', 'is', 'lack', 'leads', 'lie', 'life', 'lord', 'love', 'makes', 'me', 'my', 'name', 'no', 'nothing', 'of', 'oil', 'overflows', 'pastures', 'paths', 'prepare', 'presence', 'quiet', 'refreshes', 'right', 'rod', 'sake', 'shepherd', 'soul', 'staff', 'surely', 'table', 'the', 'they', 'though', 'through', 'valley', 'walk', 'waters', 'will', 'with', 'you', 'your']\n",
      "[[1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 4 1 1 1 3 1 1 1 1 1 2 1 1\n",
      "  7 6 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 7 1 1 1 1 1 1 3 2 3 3]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "print(type(cvec))\n",
    "print(cvec.get_feature_names())\n",
    "print(ps23_Counts.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.matrixlib.defmatrix.matrix"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ps23_Counts.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.max(ps23_Counts.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0], dtype=int64), array([36, 58], dtype=int64))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(ps23_Counts.todense() == np.max(ps23_Counts.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('me', 'the')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec.get_feature_names()[36], cvec.get_feature_names()[58]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode other documents that contain words that are not included in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps34 = ['I will extol the LORD at all times; his praise will always be on my lips. I will glory in the LORD; let the afflicted hear and rejoice. Glorify the LORD with me; let us exalt his name together. I sought the LORD, and he answered me; he delivered me from all my fears. 5 Those who look to him are radiant; their faces are never covered with shame. 6 This poor man called, and the LORD heard him; he saved him out of all his troubles.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps34_Counts = cvec.transform(ps34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'along', 'and', 'anoint', 'are', 'before', 'beside', 'comfort', 'cup', 'darkest', 'days', 'down', 'dwell', 'enemies', 'even', 'evil', 'fear', 'follow', 'for', 'forever', 'goodness', 'green', 'guides', 'he', 'head', 'his', 'house', 'in', 'is', 'lack', 'leads', 'lie', 'life', 'lord', 'love', 'makes', 'me', 'my', 'name', 'no', 'nothing', 'of', 'oil', 'overflows', 'pastures', 'paths', 'prepare', 'presence', 'quiet', 'refreshes', 'right', 'rod', 'sake', 'shepherd', 'soul', 'staff', 'surely', 'table', 'the', 'they', 'though', 'through', 'valley', 'walk', 'waters', 'will', 'with', 'you', 'your']\n",
      "[[3 0 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 3 0 1 0 0 0 0 0 5 0 0\n",
      "  3 2 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 3 2 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(cvec.get_feature_names())\n",
    "print(ps34_Counts.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_transform() vs transform() \n",
    "- fit_transform(raw_documents[, y])\tLearn the vocabulary dictionary and return term-document matrix. When we do fit_transform(), the vocabulary is learn from the document and document matrix is returned. In our previous case, we want the system to learn the vocab from the Ps23 text and create the term document matrix for ps34. In this case, we cannot use fit_transform(). Hence, we use just transform(). Also fit_transform() is the combined step of fit() and transform().  \n",
    "- fit(raw_documents[, y])\tLearn a vocabulary dictionary of all tokens in the raw documents.  \n",
    "- transform(raw_documents)\tTransform documents to document-term matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps23 = ['ps23 is cool', 'ps34 is super cool']\n",
    "bigramvect = CountVectorizer(ngram_range=(1,2), token_pattern= r'\\b\\w+\\b', min_df=1)\n",
    "ps23_bi = bigramvect.fit_transform(ps23).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps23_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps23_bi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ps23': 4, 'is': 1, 'cool': 0, 'ps23 is': 5, 'is cool': 2, 'ps34': 6, 'super': 8, 'ps34 is': 7, 'is super': 3, 'super cool': 9}\n"
     ]
    }
   ],
   "source": [
    "print(bigramvect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cool', 'is', 'is cool', 'is super', 'ps23', 'ps23 is', 'ps34', 'ps34 is', 'super', 'super cool']\n"
     ]
    }
   ],
   "source": [
    "print(bigramvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramvect.vocabulary_.get('is cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps23_bi[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Word Frequencies with TfidfVectorizer\n",
    "Issue with word counts is that some words like “the” will appear many times and their large counts will not be very meaningful in the encoded vectors.\n",
    "\n",
    "An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for “Term Frequency – Inverse Document” Frequency which are the components of the resulting scores assigned to each word.\n",
    "\n",
    "Term Frequency: This summarizes how often a given word appears within a document.\n",
    "Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "Without going into the math, TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n",
    "\n",
    "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfvec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = [\"The quick brown fox jumped over the lazy dog.\", \n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "text_tf = tfvec.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36388646, 0.27674503, 0.27674503, 0.36388646, 0.36388646,\n",
       "        0.36388646, 0.36388646, 0.42983441],\n",
       "       [0.        , 0.78980693, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.61335554],\n",
       "       [0.        , 0.        , 0.78980693, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.61335554]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "print(tfvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(tfvec.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36388646, 0.27674503, 0.27674503, 0.36388646, 0.36388646,\n",
       "        0.36388646, 0.36388646, 0.42983441],\n",
       "       [0.        , 0.78980693, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.61335554],\n",
       "       [0.        , 0.        , 0.78980693, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.61335554]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are normalized to values between 0 and 1 and the encoded document vectors can then be used directly with most machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Hashing with HashingVectorizer\n",
    "Counts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large.\n",
    "\n",
    "This, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms.\n",
    "\n",
    "A clever work around is to use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks).\n",
    "\n",
    "The HashingVectorizer class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text document\n",
    "text = ['The quick brown fox jumped over the lazy dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Convert a collection of text documents to a matrix of token occurrences\n",
    "hvec = HashingVectorizer(n_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_hv = hvec.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.33333333,  0.        , -0.33333333,  0.33333333,  0.        ,\n",
       "         0.        ,  0.33333333,  0.        ,  0.        ,  0.        ,\n",
       "        -0.33333333,  0.        ,  0.        , -0.66666667,  0.        ]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_hv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Examples - Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab1 = open('102welcome.txt').read().replace('\\n', '')\n",
    "vocab2 = open('103welcome.txt').read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'welcome to Big Data Analytics. My name is Ziyad Al-Khinali and I will be your instructor for this course. To learn more about my background, check out Meet Your Instructor, under the About the Course module. This course will have a lab coach available to assist you with the lab portion of this course and any software and application questions that you may have.  To contact the Lab Coach, use the lab coach discussion forum.  More details will follow after the start of the course.  Before delving into the course content and learning activities, it is important to be aware of the resources that will be available to you throughout the term. Use these resources to build your learning and time management strategies, so that you can meet the course expectations.Review the Getting Started module to:learn how to navigate your course (A2L Guide)learn how to avoid falling into plagiarism (Academic Integrity Guide)identify library and research resources (Library, Research and Writing Assistance)Review the About the Course module to learn how to:organize your study time and meet all due dates (Detailed Course Schedule)gather the course required materials (Required Materials)reach out when you need help (Contact Information)I also encourage you to introduce yourself to the discussion forum and connect with each other to exchange experiences, study together and discuss further what interests you most about this course. Make the time to take advantage of the networking opportunity of being part of the McMasterâ€™s CCE student community.Iâ€™m looking forward to meeting each of you in-class and guiding your learning experience this term.Ziyad, '"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome to Big Data Analytics. My name is Ziyad Al-Khinali and I will be your instructor for this course. To learn more about my background, check out Meet Your Instructor, under the About the Course module. This course will have a lab coach available to assist you with the lab portion of this course and any software and application questions that you may have.  To contact the Lab Coach, use the lab coach discussion forum.  More details will follow after the start of the course.  Before delving into the course content and learning activities, it is important to be aware of the resources that will be available to you throughout the term. Use these resources to build your learning and time management strategies, so that you can meet the course expectations.Review the Getting Started module to:learn how to navigate your course (A2L Guide)learn how to avoid falling into plagiarism (Academic Integrity Guide)identify library and research resources (Library, Research and Writing Assistance)Review the About the Course module to learn how to:organize your study time and meet all due dates (Detailed Course Schedule)gather the course required materials (Required Materials)reach out when you need help (Contact Information)I also encourage you to introduce yourself to the discussion forum and connect with each other to exchange experiences, study together and discuss further what interests you most about this course. Make the time to take advantage of the networking opportunity of being part of the McMasterâ€™s CCE student community.Iâ€™m looking forward to meeting each of you in-class and guiding your learning experience this term.Ziyad, ', \"welcome to BDA 103 Data Management.My name is Eleanor Smith and I will be your instructor for this course. To learn more about my background, check out Meet Your Instructor, under the About the Course module. Your lab coach's name is Akash Shetty and he will be assisting you with the lab portion of this course and any software and application questions that you may have. To learn more about your lab coach's background, check out Meet Your Lab Coach, under the About the Course module. Before delving into the course content and learning activities, it is important to be aware of the resources that will be available to you throughout the term. Use these resources to build your learning and time management strategies, so that you can meet the course expectations.Review the Getting Started module to:learn how to navigate your course (A2L Guide)learn how to avoid falling into plagiarism (Academic Integrity Guide)identify library and research resources (Library, Research and Writing Assistance)Review the About the Course module to learn how to:organize your study time and meet all due dates (Detailed Course Schedule)reach out when you need help (Contact Information)I also encourage you to introduce yourself in the Welcome and Introductions discussion forum and connect with each other to exchange experiences, study together and discuss further what interests you most about this course. Make the time to take advantage of the networking opportunity of being part of the McMasterâ€™s CCE student community.Iâ€™m looking forward to meeting each of you in-class and guiding your learning experience this term.- Eleanor\"]\n"
     ]
    }
   ],
   "source": [
    "train_set = [vocab1, vocab2]\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing text with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  1  1  1  0  1  1  1  1  1  0  2  1  1  1  0  1  1  1  1  1  3  1\n",
      "   1  2  1 12  1  1  1  1  1  1  2  0  1  1  1  1  1  1  1  2  1  1  1  2\n",
      "   1  1  1  1  1  2  1  1  1  0  1  1  4  4  3  2  1  1  1  2  1  3  1  3\n",
      "   1  1  1  1  1  1  1  1  1  2  2  3  2  1  0  0  1  1  1  1  1  2  2  3\n",
      "   2  1  1  2]\n",
      " [ 1  1  1  1  1  1  0  0  1  0  1  1  1  1  1  2  1  0  1  1  2  1  3  1\n",
      "   1  1  1 10  1  1  1  1  0  1  1  2  1  1  1  1  1  1  0  1  1  0  1  2\n",
      "   1  1  1  1  1  2  1  1  1  1  1  0  4  5  3  2  1  1  2  0  1  4  1  4\n",
      "   1  1  1  1  1  1  1  1  1  0  2  3  2  1  1  1  1  0  1  1  1  2  2  3\n",
      "   1  2  1  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(stop_words='english') ###Convert a collection of text documents to a matrix of token counts\n",
    "X_train_counts = vec.fit_transform(train_set) ### Term document matrix\n",
    "print(X_train_counts.toarray()) ### Return a dense matrix representation of this matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts.shape) ##There are 2 documents and 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['103', 'a2l', 'academic', 'activities', 'advantage', 'akash', 'al', 'analytics', 'application', 'assist', 'assistance', 'assisting', 'available', 'avoid', 'aware', 'background', 'bda', 'big', 'build', 'cce', 'check', 'class', 'coach', 'community', 'connect', 'contact', 'content', 'course', 'data', 'dates', 'delving', 'detailed', 'details', 'discuss', 'discussion', 'eleanor', 'encourage', 'exchange', 'expectations', 'experience', 'experiences', 'falling', 'follow', 'forum', 'forward', 'gather', 'getting', 'guide', 'guiding', 'help', 'identify', 'important', 'information', 'instructor', 'integrity', 'interests', 'introduce', 'introductions', 'iâ', 'khinali', 'lab', 'learn', 'learning', 'library', 'looking', 'make', 'management', 'materials', 'mcmasterâ', 'meet', 'meeting', 'module', 'navigate', 'need', 'networking', 'opportunity', 'organize', 'plagiarism', 'portion', 'questions', 'reach', 'required', 'research', 'resources', 'review', 'schedule', 'shetty', 'smith', 'software', 'start', 'started', 'strategies', 'student', 'study', 'term', 'time', 'use', 'welcome', 'writing', 'ziyad']\n"
     ]
    }
   ],
   "source": [
    "print(vec.get_feature_names()) ## Array mapping from feature integer indices to feature name\n",
    "## This lists the list of words aka bag of words in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'welcome': 97, 'big': 17, 'data': 28, 'analytics': 7, 'ziyad': 99, 'al': 6, 'khinali': 59, 'instructor': 53, 'course': 27, 'learn': 61, 'background': 15, 'check': 20, 'meet': 69, 'module': 71, 'lab': 60, 'coach': 22, 'available': 12, 'assist': 9, 'portion': 78, 'software': 88, 'application': 8, 'questions': 79, 'contact': 25, 'use': 96, 'discussion': 34, 'forum': 43, 'details': 32, 'follow': 42, 'start': 89, 'delving': 30, 'content': 26, 'learning': 62, 'activities': 3, 'important': 51, 'aware': 14, 'resources': 83, 'term': 94, 'build': 18, 'time': 95, 'management': 66, 'strategies': 91, 'expectations': 38, 'review': 84, 'getting': 46, 'started': 90, 'navigate': 72, 'a2l': 1, 'guide': 47, 'avoid': 13, 'falling': 41, 'plagiarism': 77, 'academic': 2, 'integrity': 54, 'identify': 50, 'library': 63, 'research': 82, 'writing': 98, 'assistance': 10, 'organize': 76, 'study': 93, 'dates': 29, 'detailed': 31, 'schedule': 85, 'gather': 45, 'required': 81, 'materials': 67, 'reach': 80, 'need': 73, 'help': 49, 'information': 52, 'encourage': 36, 'introduce': 56, 'connect': 24, 'exchange': 37, 'experiences': 40, 'discuss': 33, 'interests': 55, 'make': 65, 'advantage': 4, 'networking': 74, 'opportunity': 75, 'mcmasterâ': 68, 'cce': 19, 'student': 92, 'community': 23, 'iâ': 58, 'looking': 64, 'forward': 44, 'meeting': 70, 'class': 21, 'guiding': 48, 'experience': 39, 'bda': 16, '103': 0, 'eleanor': 35, 'smith': 87, 'akash': 5, 'shetty': 86, 'assisting': 11, 'introductions': 57}\n"
     ]
    }
   ],
   "source": [
    "print(vec.vocabulary_) ## vocabulary_ : dict -> A mapping of terms to feature indices.\n",
    "## This lists all the words and the corresponding indices. For instance welcome has the index 97 \n",
    "## Note: there are 100 total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec.get_feature_names()), len(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set = train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are trying to  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  1  1  1  0  1  1  1  1  1  0  2  1  1  1  0  1  1  1  1  1  3  1\n",
      "   1  2  1 12  1  1  1  1  1  1  2  0  1  1  1  1  1  1  1  2  1  1  1  2\n",
      "   1  1  1  1  1  2  1  1  1  0  1  1  4  4  3  2  1  1  1  2  1  3  1  3\n",
      "   1  1  1  1  1  1  1  1  1  2  2  3  2  1  0  0  1  1  1  1  1  2  2  3\n",
      "   2  1  1  2]\n",
      " [ 1  1  1  1  1  1  0  0  1  0  1  1  1  1  1  2  1  0  1  1  2  1  3  1\n",
      "   1  1  1 10  1  1  1  1  0  1  1  2  1  1  1  1  1  1  0  1  1  0  1  2\n",
      "   1  1  1  1  1  2  1  1  1  1  1  0  4  5  3  2  1  1  2  0  1  4  1  4\n",
      "   1  1  1  1  1  1  1  1  1  0  2  3  2  1  1  1  1  0  1  1  1  2  2  3\n",
      "   1  2  1  0]]\n"
     ]
    }
   ],
   "source": [
    "X_test_counts = vec.transform(test_set)\n",
    "print(X_test_counts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sum 2 rows and build a frequency table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>103</th>\n",
       "      <th>a2l</th>\n",
       "      <th>academic</th>\n",
       "      <th>activities</th>\n",
       "      <th>advantage</th>\n",
       "      <th>akash</th>\n",
       "      <th>al</th>\n",
       "      <th>analytics</th>\n",
       "      <th>application</th>\n",
       "      <th>assist</th>\n",
       "      <th>...</th>\n",
       "      <th>started</th>\n",
       "      <th>strategies</th>\n",
       "      <th>student</th>\n",
       "      <th>study</th>\n",
       "      <th>term</th>\n",
       "      <th>time</th>\n",
       "      <th>use</th>\n",
       "      <th>welcome</th>\n",
       "      <th>writing</th>\n",
       "      <th>ziyad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   103  a2l  academic  activities  advantage  akash  al  analytics  \\\n",
       "0    0    1         1           1          1      0   1          1   \n",
       "1    1    1         1           1          1      1   0          0   \n",
       "\n",
       "   application  assist  ...    started  strategies  student  study  term  \\\n",
       "0            1       1  ...          1           1        1      2     2   \n",
       "1            1       0  ...          1           1        1      2     2   \n",
       "\n",
       "   time  use  welcome  writing  ziyad  \n",
       "0     3    2        1        1      2  \n",
       "1     3    1        2        1      0  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(X_test_counts.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sumdf = df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sumdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a2l</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>academic</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activities</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advantage</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>akash</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>al</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytics</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>application</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assist</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assistance</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assisting</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>available</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avoid</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aware</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>background</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bda</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>build</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cce</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coach</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>community</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>connect</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contact</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>course</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dates</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meeting</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>module</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>navigate</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networking</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opportunity</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>organize</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plagiarism</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portion</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>questions</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reach</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>required</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resources</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schedule</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shetty</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smith</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>started</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strategies</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>welcome</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writing</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ziyad</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "103           1\n",
       "a2l           2\n",
       "academic      2\n",
       "activities    2\n",
       "advantage     2\n",
       "akash         1\n",
       "al            1\n",
       "analytics     1\n",
       "application   2\n",
       "assist        1\n",
       "assistance    2\n",
       "assisting     1\n",
       "available     3\n",
       "avoid         2\n",
       "aware         2\n",
       "background    3\n",
       "bda           1\n",
       "big           1\n",
       "build         2\n",
       "cce           2\n",
       "check         3\n",
       "class         2\n",
       "coach         6\n",
       "community     2\n",
       "connect       2\n",
       "contact       3\n",
       "content       2\n",
       "course       22\n",
       "data          2\n",
       "dates         2\n",
       "...          ..\n",
       "meeting       2\n",
       "module        7\n",
       "navigate      2\n",
       "need          2\n",
       "networking    2\n",
       "opportunity   2\n",
       "organize      2\n",
       "plagiarism    2\n",
       "portion       2\n",
       "questions     2\n",
       "reach         2\n",
       "required      2\n",
       "research      4\n",
       "resources     6\n",
       "review        4\n",
       "schedule      2\n",
       "shetty        1\n",
       "smith         1\n",
       "software      2\n",
       "start         1\n",
       "started       2\n",
       "strategies    2\n",
       "student       2\n",
       "study         4\n",
       "term          4\n",
       "time          6\n",
       "use           3\n",
       "welcome       3\n",
       "writing       2\n",
       "ziyad         2\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sumdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vocab</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2l</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>academic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>activities</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>advantage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>akash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>al</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>analytics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>application</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>assist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>assistance</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>assisting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>available</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>avoid</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>aware</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>background</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bda</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>big</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>build</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cce</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>check</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>class</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>coach</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>community</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>connect</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>contact</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>content</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>course</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>data</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>dates</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>meeting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>module</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>navigate</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>need</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>networking</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>opportunity</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>organize</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>plagiarism</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>portion</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>questions</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>reach</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>required</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>research</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>resources</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>review</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>schedule</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>shetty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>smith</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>software</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>start</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>started</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>strategies</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>student</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>study</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>term</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>time</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>use</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>welcome</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>writing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ziyad</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Vocab  Frequency\n",
       "0           103          1\n",
       "1           a2l          2\n",
       "2      academic          2\n",
       "3    activities          2\n",
       "4     advantage          2\n",
       "5         akash          1\n",
       "6            al          1\n",
       "7     analytics          1\n",
       "8   application          2\n",
       "9        assist          1\n",
       "10   assistance          2\n",
       "11    assisting          1\n",
       "12    available          3\n",
       "13        avoid          2\n",
       "14        aware          2\n",
       "15   background          3\n",
       "16          bda          1\n",
       "17          big          1\n",
       "18        build          2\n",
       "19          cce          2\n",
       "20        check          3\n",
       "21        class          2\n",
       "22        coach          6\n",
       "23    community          2\n",
       "24      connect          2\n",
       "25      contact          3\n",
       "26      content          2\n",
       "27       course         22\n",
       "28         data          2\n",
       "29        dates          2\n",
       "..          ...        ...\n",
       "70      meeting          2\n",
       "71       module          7\n",
       "72     navigate          2\n",
       "73         need          2\n",
       "74   networking          2\n",
       "75  opportunity          2\n",
       "76     organize          2\n",
       "77   plagiarism          2\n",
       "78      portion          2\n",
       "79    questions          2\n",
       "80        reach          2\n",
       "81     required          2\n",
       "82     research          4\n",
       "83    resources          6\n",
       "84       review          4\n",
       "85     schedule          2\n",
       "86       shetty          1\n",
       "87        smith          1\n",
       "88     software          2\n",
       "89        start          1\n",
       "90      started          2\n",
       "91   strategies          2\n",
       "92      student          2\n",
       "93        study          4\n",
       "94         term          4\n",
       "95         time          6\n",
       "96          use          3\n",
       "97      welcome          3\n",
       "98      writing          2\n",
       "99        ziyad          2\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'Vocab': sumdf.index, 'Frequency': sumdf.values})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train1 = [vocab1]\n",
    "test1 = [vocab2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  3  1  1  2  1 12\n",
      "   1  1  1  1  1  1  2  1  1  1  1  1  1  1  2  1  1  1  2  1  1  1  1  1\n",
      "   2  1  1  1  1  1  4  4  3  2  1  1  1  2  1  3  1  3  1  1  1  1  1  1\n",
      "   1  1  1  2  2  3  2  1  1  1  1  1  1  2  2  3  2  1  1  2]]\n"
     ]
    }
   ],
   "source": [
    "cvec = CountVectorizer(stop_words='english')\n",
    "X_train1_counts = cvec.fit_transform(train1)\n",
    "print(X_train1_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'welcome': 97, 'big': 17, 'data': 28, 'analytics': 7, 'ziyad': 99, 'al': 6, 'khinali': 59, 'instructor': 53, 'course': 27, 'learn': 61, 'background': 15, 'check': 20, 'meet': 69, 'module': 71, 'lab': 60, 'coach': 22, 'available': 12, 'assist': 9, 'portion': 78, 'software': 88, 'application': 8, 'questions': 79, 'contact': 25, 'use': 96, 'discussion': 34, 'forum': 43, 'details': 32, 'follow': 42, 'start': 89, 'delving': 30, 'content': 26, 'learning': 62, 'activities': 3, 'important': 51, 'aware': 14, 'resources': 83, 'term': 94, 'build': 18, 'time': 95, 'management': 66, 'strategies': 91, 'expectations': 38, 'review': 84, 'getting': 46, 'started': 90, 'navigate': 72, 'a2l': 1, 'guide': 47, 'avoid': 13, 'falling': 41, 'plagiarism': 77, 'academic': 2, 'integrity': 54, 'identify': 50, 'library': 63, 'research': 82, 'writing': 98, 'assistance': 10, 'organize': 76, 'study': 93, 'dates': 29, 'detailed': 31, 'schedule': 85, 'gather': 45, 'required': 81, 'materials': 67, 'reach': 80, 'need': 73, 'help': 49, 'information': 52, 'encourage': 36, 'introduce': 56, 'connect': 24, 'exchange': 37, 'experiences': 40, 'discuss': 33, 'interests': 55, 'make': 65, 'advantage': 4, 'networking': 74, 'opportunity': 75, 'mcmasterâ': 68, 'cce': 19, 'student': 92, 'community': 23, 'iâ': 58, 'looking': 64, 'forward': 44, 'meeting': 70, 'class': 21, 'guiding': 48, 'experience': 39, 'bda': 16, '103': 0, 'eleanor': 35, 'smith': 87, 'akash': 5, 'shetty': 86, 'assisting': 11, 'introductions': 57}\n"
     ]
    }
   ],
   "source": [
    "print(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['103', 'a2l', 'academic', 'activities', 'advantage', 'akash', 'al', 'analytics', 'application', 'assist', 'assistance', 'assisting', 'available', 'avoid', 'aware', 'background', 'bda', 'big', 'build', 'cce', 'check', 'class', 'coach', 'community', 'connect', 'contact', 'content', 'course', 'data', 'dates', 'delving', 'detailed', 'details', 'discuss', 'discussion', 'eleanor', 'encourage', 'exchange', 'expectations', 'experience', 'experiences', 'falling', 'follow', 'forum', 'forward', 'gather', 'getting', 'guide', 'guiding', 'help', 'identify', 'important', 'information', 'instructor', 'integrity', 'interests', 'introduce', 'introductions', 'iâ', 'khinali', 'lab', 'learn', 'learning', 'library', 'looking', 'make', 'management', 'materials', 'mcmasterâ', 'meet', 'meeting', 'module', 'navigate', 'need', 'networking', 'opportunity', 'organize', 'plagiarism', 'portion', 'questions', 'reach', 'required', 'research', 'resources', 'review', 'schedule', 'shetty', 'smith', 'software', 'start', 'started', 'strategies', 'student', 'study', 'term', 'time', 'use', 'welcome', 'writing', 'ziyad']\n"
     ]
    }
   ],
   "source": [
    "print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  1  1  1  0  0  1  0  1  1  1  1  1  2  1  0  1  1  2  1  3  1\n",
      "   1  1  1 10  1  1  1  1  0  1  1  2  1  1  1  1  1  1  0  1  1  0  1  2\n",
      "   1  1  1  1  1  2  1  1  1  1  1  0  4  5  3  2  1  1  2  0  1  4  1  4\n",
      "   1  1  1  1  1  1  1  1  1  0  2  3  2  1  1  1  1  0  1  1  1  2  2  3\n",
      "   1  2  1  0]]\n"
     ]
    }
   ],
   "source": [
    "X_test1_counts = vec.transform(test1)\n",
    "print(X_test1_counts.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
